{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataProcessing import *\n",
    "from FeatureEngineering import *\n",
    "from Tuning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate data object $data$\n",
    "- assign ordinal categorical features \n",
    "- separate full train data into train, test\n",
    "\n",
    "#### Vectorize categorical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>milesFromMetropolis</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>jobId</th>\n",
       "      <th>salary</th>\n",
       "      <th>companyId</th>\n",
       "      <th>industry</th>\n",
       "      <th>major</th>\n",
       "      <th>degree</th>\n",
       "      <th>jobType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>JOB1362685396294</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>JOB1362684596757</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>JOB1362685320583</td>\n",
       "      <td>125</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>JOB1362685350076</td>\n",
       "      <td>129</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>JOB1362685172779</td>\n",
       "      <td>99</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   milesFromMetropolis  yearsExperience             jobId  salary  companyId  \\\n",
       "0                   39                3  JOB1362685396294     103         30   \n",
       "1                   39               23  JOB1362684596757     191         51   \n",
       "2                   10               19  JOB1362685320583     125         53   \n",
       "3                   22               24  JOB1362685350076     129         13   \n",
       "4                   21               22  JOB1362685172779      99         45   \n",
       "\n",
       "   industry  major  degree  jobType  \n",
       "0         2      7       0        3  \n",
       "1         3      8       3        7  \n",
       "2         0      7       2        6  \n",
       "3         3      7       0        6  \n",
       "4         1      7       0        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(699997, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinalcodes = {'degree':{'NONE':0,\n",
    "                          'HIGH_SCHOOL':1,\n",
    "                          'BACHELORS':2,\n",
    "                          'MASTERS':3,\n",
    "                          'DOCTORAL':4},\n",
    "                'jobType':{'JANITOR':0,\n",
    "                           'JUNIOR':1,\n",
    "                           'SENIOR':2,\n",
    "                           'MANAGER':3,\n",
    "                           'VICE_PRESIDENT':4,\n",
    "                           'CFO':5,\n",
    "                           'CTO':6,\n",
    "                           'CEO':7}\n",
    "               }\n",
    "\n",
    "\n",
    "data = Data('train_features.csv','train_salaries.csv',\n",
    "         ['companyId','degree','industry','jobType','major'],\n",
    "         ['milesFromMetropolis','yearsExperience'],\n",
    "         'salary','jobId',ordinalcodes,test_sz=.3)\n",
    "\n",
    "train_df = data._preprocessData(data.train_df,\n",
    "                                True,True,\n",
    "                                ['companyId','industry','major'])\n",
    "# assign copy of train data in initial vectorized state\n",
    "vec_train_df = train_df.copy()\n",
    "display(train_df.head())\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate feature engineering object and compute new response statistics on grouped features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>milesFromMetropolis</th>\n",
       "      <th>yearsExperience</th>\n",
       "      <th>jobId</th>\n",
       "      <th>salary</th>\n",
       "      <th>companyId</th>\n",
       "      <th>industry</th>\n",
       "      <th>major</th>\n",
       "      <th>degree</th>\n",
       "      <th>jobType</th>\n",
       "      <th>cat_group_min</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_group_mean</th>\n",
       "      <th>cat_group_max</th>\n",
       "      <th>cat_group_std</th>\n",
       "      <th>milesFromMetropolis_quantile</th>\n",
       "      <th>yearsExperience_quantile</th>\n",
       "      <th>numeric_quantile_min</th>\n",
       "      <th>numeric_quantile_median</th>\n",
       "      <th>numeric_quantile_mean</th>\n",
       "      <th>numeric_quantile_max</th>\n",
       "      <th>numeric_quantile_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>JOB1362685396294</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>116.822222</td>\n",
       "      <td>209</td>\n",
       "      <td>34.350658</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>102</td>\n",
       "      <td>103.019774</td>\n",
       "      <td>245</td>\n",
       "      <td>39.274601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>JOB1362684596757</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>170.250000</td>\n",
       "      <td>191</td>\n",
       "      <td>18.997807</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>143</td>\n",
       "      <td>145.250973</td>\n",
       "      <td>284</td>\n",
       "      <td>43.013118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>JOB1362685320583</td>\n",
       "      <td>125</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>124.285714</td>\n",
       "      <td>140</td>\n",
       "      <td>11.513967</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>159</td>\n",
       "      <td>160.419643</td>\n",
       "      <td>301</td>\n",
       "      <td>44.190884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>JOB1362685350076</td>\n",
       "      <td>129</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>119.727273</td>\n",
       "      <td>213</td>\n",
       "      <td>27.278379</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>159</td>\n",
       "      <td>160.419643</td>\n",
       "      <td>301</td>\n",
       "      <td>44.190884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>JOB1362685172779</td>\n",
       "      <td>99</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>53.450980</td>\n",
       "      <td>102</td>\n",
       "      <td>19.952234</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>159</td>\n",
       "      <td>160.419643</td>\n",
       "      <td>301</td>\n",
       "      <td>44.190884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   milesFromMetropolis  yearsExperience             jobId  salary  companyId  \\\n",
       "0                   39                3  JOB1362685396294     103         30   \n",
       "1                   39               23  JOB1362684596757     191         51   \n",
       "2                   10               19  JOB1362685320583     125         53   \n",
       "3                   22               24  JOB1362685350076     129         13   \n",
       "4                   21               22  JOB1362685172779      99         45   \n",
       "\n",
       "   industry  major  degree  jobType  cat_group_min  ...  cat_group_mean  \\\n",
       "0         2      7       0        3             66  ...      116.822222   \n",
       "1         3      8       3        7            150  ...      170.250000   \n",
       "2         0      7       2        6            108  ...      124.285714   \n",
       "3         3      7       0        6             82  ...      119.727273   \n",
       "4         1      7       0        0             23  ...       53.450980   \n",
       "\n",
       "   cat_group_max  cat_group_std  milesFromMetropolis_quantile  \\\n",
       "0            209      34.350658                             1   \n",
       "1            191      18.997807                             1   \n",
       "2            140      11.513967                             0   \n",
       "3            213      27.278379                             0   \n",
       "4            102      19.952234                             0   \n",
       "\n",
       "   yearsExperience_quantile  numeric_quantile_min  numeric_quantile_median  \\\n",
       "0                         0                    23                      102   \n",
       "1                         3                    49                      143   \n",
       "2                         3                    62                      159   \n",
       "3                         3                    62                      159   \n",
       "4                         3                    62                      159   \n",
       "\n",
       "   numeric_quantile_mean  numeric_quantile_max  numeric_quantile_std  \n",
       "0             103.019774                   245             39.274601  \n",
       "1             145.250973                   284             43.013118  \n",
       "2             160.419643                   301             44.190884  \n",
       "3             160.419643                   301             44.190884  \n",
       "4             160.419643                   301             44.190884  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(699997, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureEng = FeatureEng(data) \n",
    "\n",
    "train_df = featureEng._compute_new_features(vec_train_df,train_df,\n",
    "                                            featureEng.data.columns_cat,\n",
    "                                            featureEng.data.columns_num,4)\n",
    "display(train_df.head())\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate $tune$ object which contains linear estimator and hyperparameter tuning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = []\n",
    "loss_dict = {}\n",
    "\n",
    "tune = Tuning(train_df.drop(data.id,axis=1),\n",
    "              data.ylabel,5,model_list,\n",
    "              loss_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor\n",
    "\n",
    "RandomForestRegressor hyperparameters:\n",
    "- ***n_estimators*** controls the number of trees in the forest. i.e. the number of resampled, bagged trees to include in the (averaged) forest. Large *n_estimators* is computationally expensive. Random search found best *n_estimators* to be around 58.\n",
    "- ***max_depth*** controls the maximum depth of the tree. Larger *max_depth* increases variance and reduces bias.\n",
    "- ***min_samples_split*** The minimum number of samples required to split a node. Lower values increases variance and reduces bias. Random search found best *max_depth* to be around 58.\n",
    "- ***min_samples_leaf*** controls the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. Lower values (float) increases variance and reduces bias. Random search found best *min_samples_leaf* value to be around 92.\n",
    "- ***min_weight_fraction_leaf*** is minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Lower values increases variance and reduces bias. At the cost of possible loss improvement, *min_weight_fraction_leaf* was left at the default value of 0 in order to save time. \n",
    "- ***max_features*** specifies the number of features to consider when looking for the best split. Random search found ***max_fratures*** to be around 19.\n",
    "***min_impurity_decrease*** controls when a node will be split if this split induces a decrease of the impurity greater than or equal to this value. Lower values increases variance, reduces bias. Random search found best *min_impurity_decrease* to be around 0.017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial RandomSearchCV search for RandomForest hyperparameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned optimal hyperparmeter:\n",
      "randomforestregressor__max_depth: 71\n",
      "randomforestregressor__max_features: 0.865505794816589\n",
      "randomforestregressor__max_leaf_nodes: None\n",
      "randomforestregressor__min_impurity_decrease: 0.03631976823954457\n",
      "randomforestregressor__min_samples_leaf: 84\n",
      "randomforestregressor__min_samples_split: 7\n",
      "randomforestregressor__n_estimators: 32\n",
      "\n",
      "gridsearch best score: 311.1461223626699\n",
      "Time to complete:25.596480703353883\n"
     ]
    }
   ],
   "source": [
    "tune = Tuning(train_df.drop(data.id,axis=1),\n",
    "              data.ylabel,5,\n",
    "              model_list,loss_dict)\n",
    "\n",
    "t0 = time()\n",
    "rf_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                        RandomForestRegressor())\n",
    "rf_pipe_best = tune._tune_hyperparams(model=rf_pipe,params={\n",
    "                                            'randomforestregressor__n_estimators':randint(2,100), \n",
    "                                            'randomforestregressor__max_depth':randint(2,100),  \n",
    "                                            'randomforestregressor__min_samples_split':randint(2,100), \n",
    "                                            'randomforestregressor__min_samples_leaf':randint(2,100), \n",
    "                                            'randomforestregressor__max_features':uniform(0,.99),\n",
    "                                            'randomforestregressor__max_leaf_nodes':[None], \n",
    "                                            'randomforestregressor__min_impurity_decrease':uniform(0,.05), \n",
    "                                                   },lossfunc='neg_mean_squared_error',figsz=(10,4),ylabel=data.ylabel,\n",
    "                                   plot_param=None,\n",
    "                                   searchRandom=True,n_iter=10,verbose=False)\n",
    "\n",
    "print('Time to complete:{}'.format((time()-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow the range of hyperparameters suggested by initial RandomizedSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned optimal hyperparmeter:\n",
      "randomforestregressor__max_depth: 62\n",
      "randomforestregressor__max_features: 19\n",
      "randomforestregressor__max_leaf_nodes: None\n",
      "randomforestregressor__min_impurity_decrease: 0.01706250684255796\n",
      "randomforestregressor__min_samples_leaf: 88\n",
      "randomforestregressor__min_samples_split: 11\n",
      "randomforestregressor__n_estimators: 38\n",
      "\n",
      "gridsearch best score: 305.613767505715\n",
      "Time to complete:128.114544403553\n"
     ]
    }
   ],
   "source": [
    "tune = Tuning(train_df.drop(data.id,axis=1),\n",
    "              data.ylabel,5,\n",
    "              model_list,loss_dict)\n",
    "\n",
    "t0 = time()\n",
    "rf_pipe = make_pipeline(SimpleImputer(strategy='median'),RandomForestRegressor())\n",
    "rf_pipe_best = tune._tune_hyperparams(model=rf_pipe,params={\n",
    "                                            'randomforestregressor__n_estimators':randint(22,42), \n",
    "                                            'randomforestregressor__max_depth':randint(61,81),  \n",
    "                                            'randomforestregressor__min_samples_split':randint(2,12), \n",
    "                                            'randomforestregressor__min_samples_leaf':randint(78,89), \n",
    "                                            'randomforestregressor__max_features':randint(16,20),\n",
    "                                            'randomforestregressor__max_leaf_nodes':[None], \n",
    "                                            'randomforestregressor__min_impurity_decrease':uniform(.015,.05), \n",
    "                                                   },lossfunc='neg_mean_squared_error',figsz=(10,4),ylabel=data.ylabel,\n",
    "                                   plot_param=None,\n",
    "                                   searchRandom=True,n_iter=5,verbose=False)\n",
    "\n",
    "print('Time to complete:{}'.format((time()-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final hyperparameter search for RandomForestRegressor \n",
    "- MSE improved slightly, so we'll stop here and use the following hyperparameters in RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned optimal hyperparmeter:\n",
      "randomforestregressor__max_depth: 58\n",
      "randomforestregressor__max_features: 19\n",
      "randomforestregressor__max_leaf_nodes: None\n",
      "randomforestregressor__min_impurity_decrease: 0.01716494628293873\n",
      "randomforestregressor__min_samples_leaf: 92\n",
      "randomforestregressor__min_samples_split: 13\n",
      "randomforestregressor__n_estimators: 41\n",
      "\n",
      "gridsearch best score: 305.1139205249845\n",
      "Time to complete:137.73217782974243\n"
     ]
    }
   ],
   "source": [
    "rf_pipe = make_pipeline(SimpleImputer(strategy='median'),RandomForestRegressor())\n",
    "t0 = time()\n",
    "rf_pipe_best = tune._tune_hyperparams(model=rf_pipe,params={\n",
    "                                            'randomforestregressor__n_estimators':randint(33,43), \n",
    "                                            'randomforestregressor__max_depth':randint(57,67),  \n",
    "                                            'randomforestregressor__min_samples_split':randint(8,14), \n",
    "                                            'randomforestregressor__min_samples_leaf':randint(83,93), \n",
    "                                            'randomforestregressor__max_features':randint(16,21),\n",
    "                                            'randomforestregressor__max_leaf_nodes':[None], \n",
    "                                            'randomforestregressor__min_impurity_decrease':uniform(.015,.019), \n",
    "                                                   },lossfunc='neg_mean_squared_error',figsz=(10,4),ylabel=data.ylabel,\n",
    "                                   plot_param=None,\n",
    "                                   searchRandom=True,n_iter=4,verbose=False)\n",
    "\n",
    "print('Time to complete:{}'.format((time()-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor\n",
    "\n",
    "#### GradientBoostingRegressor hyperparameters:\n",
    "- ***learning_rate*** shrinks the contribution of each tree by the *learning_rate*. There is a trade-off between *learning_rate* and *n_estimators*. Larger values increase variance and reduce bias. Random search found best *learning_rate* to be around 0.135.\n",
    "- ***n_estimators*** is the number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Larger values of *n_estimators* increases model variance and reduces bias. Random search found best *n_estimators* value to be around 268.\n",
    "- ***subsample*** controls the fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. *subsample* interacts with the parameter *n_estimators*. *subsample* values < 1.0 reduces variance and increases bias. Random search found best *subsample* value to be around 0.68 (estimator was regulated for variance).\n",
    "- ***min_samples_split*** is the minimum number of samples required to split an internal node: If *int*, then consider *min_samples_split* as the minimum number. Larger values increase variance and reduce bias. If *float*, then *min_samples_split* is a fraction and *ceil(min_samples_split * n_samples)* are the minimum number of samples for each split. Random search found best *min_samples_split* to be around 72.\n",
    "- ***min_samples_leaf***  hyperparameter is the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least *min_samples_leaf* training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. Smaller values increase variance and reduce bias. Random search found best *min_samples_leaf* to be around 411. \n",
    "- ***min_weight_fraction_leaf*** is the minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when *sample_weight* is not provided. Smaller values increase variance and reduce bias. Random search found best *min_weigh_fraction_leaf* to be around .048. \n",
    "- ***max_depth*** specifies the maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. The best value depends on the interaction of the input variables. Larger values increase variance and decrease bias. Random search found best *max_depth* to be around 544.\n",
    "- ***min_impurity_decrease*** specifies when a node will be split if this split induces a decrease of the impurity greater than or equal to this value. Larger values increase variance and decrease bias. Random search determined the best *min_impurity_decrease* value to be around .0035. \n",
    "- ***max_features*** specifies the number of features to consider when looking for the best split. Choosing *max_features* < *n_features* leads to a reduction of variance and an increase in bias. Larger values increase variance and decrease bias. Random search found best *max_features* value to be around 8 (i.e. 8/19 ~= 42% of features were considered for each node split).\n",
    "- ***max_leaf_nodes*** controls the number of leaves in a tree. Best nodes are defined as relative reduction in impurity. If value is *None* then unlimited number of leaf nodes. Larger values increase variance and decrease bias. Random search found best *max_leaf_nodes* to be around 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor hyperparameter tuning with RandomizedSearch\n",
    "- First, search a wide range for each hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned optimal hyperparmeter:\n",
      "gradientboostingregressor__learning_rate: 0.07793043512142418\n",
      "gradientboostingregressor__max_depth: 552\n",
      "gradientboostingregressor__max_features: 10\n",
      "gradientboostingregressor__max_leaf_nodes: 18\n",
      "gradientboostingregressor__min_impurity_decrease: 0.002254503549716579\n",
      "gradientboostingregressor__min_samples_leaf: 413\n",
      "gradientboostingregressor__min_samples_split: 80\n",
      "gradientboostingregressor__min_weight_fraction_leaf: 0.04337065230411918\n",
      "gradientboostingregressor__n_estimators: 268\n",
      "gradientboostingregressor__subsample: 0.5033336861761807\n",
      "\n",
      "gridsearch best score: 300.5592771279986\n",
      "Time to execute: 236.09604076544443\n"
     ]
    }
   ],
   "source": [
    "tune = Tuning(train_df.drop(data.id,axis=1),\n",
    "              data.ylabel,5,\n",
    "              model_list,loss_dict)\n",
    "\n",
    "gb_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                        GradientBoostingRegressor())\n",
    "\n",
    "t0=time()\n",
    "gb_pipe_best = tune._tune_hyperparams(model=gb_pipe,\n",
    "                                   params={\n",
    "                                           'gradientboostingregressor__learning_rate':uniform(.06,.16),\n",
    "                                           'gradientboostingregressor__n_estimators':randint(100,300),\n",
    "                                           'gradientboostingregressor__subsample':uniform(.3,.5),\n",
    "                                           'gradientboostingregressor__min_samples_split':randint(2,100),\n",
    "                                           'gradientboostingregressor__min_samples_leaf':randint(1,800), \n",
    "                                           'gradientboostingregressor__min_weight_fraction_leaf':uniform(0,.1),#max value is .5, \n",
    "                                           'gradientboostingregressor__max_depth':randint(2,700), \n",
    "                                           'gradientboostingregressor__min_impurity_decrease':uniform(0,.01), \n",
    "                                           'gradientboostingregressor__max_features':randint(8,20),  \n",
    "                                           'gradientboostingregressor__max_leaf_nodes':randint(8,20),  \n",
    "                                           },lossfunc='neg_mean_squared_error',figsz=(10,4),ylabel=data.ylabel,\n",
    "                                   plot_param=None,\n",
    "                                   searchRandom=True,n_iter=5,verbose=False)\n",
    "\n",
    "t_elapsed = (time()-t0)/60\n",
    "print('Time to execute:',t_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune GradientBoostingRegressor hyperparameters with RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned optimal hyperparmeter:\n",
      "gradientboostingregressor__learning_rate: 0.13485569045908746\n",
      "gradientboostingregressor__max_depth: 544\n",
      "gradientboostingregressor__max_features: 8\n",
      "gradientboostingregressor__max_leaf_nodes: 17\n",
      "gradientboostingregressor__min_impurity_decrease: 0.00354711044501207\n",
      "gradientboostingregressor__min_samples_leaf: 411\n",
      "gradientboostingregressor__min_samples_split: 72\n",
      "gradientboostingregressor__min_weight_fraction_leaf: 0.04833077900157417\n",
      "gradientboostingregressor__n_estimators: 268\n",
      "gradientboostingregressor__subsample: 0.6886522918443929\n",
      "\n",
      "gridsearch best score: 298.984249922744\n",
      "Time to execute: 175.08813867171605\n"
     ]
    }
   ],
   "source": [
    "tune = Tuning(train_df.drop(data.id,axis=1),\n",
    "              data.ylabel,5,\n",
    "              model_list,loss_dict)\n",
    "\n",
    "gb_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                        GradientBoostingRegressor())\n",
    "\n",
    "t0=time()\n",
    "gb_pipe_best = tune._tune_hyperparams(model=gb_pipe,\n",
    "                                   params={\n",
    "                                           'gradientboostingregressor__learning_rate':uniform(.07,.083),\n",
    "                                           'gradientboostingregressor__n_estimators':randint(263,273),\n",
    "                                           'gradientboostingregressor__subsample':uniform(.45,.55),\n",
    "                                           'gradientboostingregressor__min_samples_split':randint(70,90),\n",
    "                                           'gradientboostingregressor__min_samples_leaf':randint(393,423), \n",
    "                                           'gradientboostingregressor__min_weight_fraction_leaf':uniform(.038,.048),#max value is .5, \n",
    "                                           'gradientboostingregressor__max_depth':randint(500,600), \n",
    "                                           'gradientboostingregressor__min_impurity_decrease':uniform(0.002,.0025), \n",
    "                                           'gradientboostingregressor__max_features':randint(8,12),  \n",
    "                                           'gradientboostingregressor__max_leaf_nodes':randint(15,22),  \n",
    "                                           },lossfunc='neg_mean_squared_error',figsz=(10,4),ylabel=data.ylabel,\n",
    "                                   plot_param=None,\n",
    "                                   searchRandom=True,n_iter=3,verbose=False)\n",
    "\n",
    "t_elapsed = (time()-t0)/60\n",
    "print('Time to execute:',t_elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
